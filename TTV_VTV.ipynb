{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ckpt_unet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 295\u001b[0m\n\u001b[1;32m    290\u001b[0m     images \u001b[39m=\u001b[39m [(image\u001b[39m.\u001b[39mnumpy() \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    291\u001b[0m               \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]  \u001b[39m# f h w c\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[39mreturn\u001b[39;00m images\n\u001b[0;32m--> 295\u001b[0m pipeline \u001b[39m=\u001b[39m TextToVideoSynthesisPipeline(\u001b[39m'\u001b[39;49m\u001b[39mtext-to-video-synthesis\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 250\u001b[0m, in \u001b[0;36mTextToVideoSynthesisPipeline.__init__\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m TextToVideoSynthesis(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mTextToVideoSynthesis.__init__\u001b[0;34m(self, model_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msd_model \u001b[39m=\u001b[39m UNetSD(\n\u001b[1;32m     56\u001b[0m     in_dim\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39munet_in_dim\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     57\u001b[0m     dim\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39munet_dim\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     dropout\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39munet_dropout\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     67\u001b[0m     temporal_attention\u001b[39m=\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39mtemporal_attention\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msd_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\n\u001b[0;32m---> 71\u001b[0m     osp\u001b[39m.\u001b[39mjoin(model_dir, cfg[\u001b[39m'\u001b[39;49m\u001b[39mckpt_unet\u001b[39;49m\u001b[39m'\u001b[39;49m])))\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msd_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msd_model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ckpt_unet'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path as osp\n",
    "from typing import Any, Dict\n",
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from einops import rearrange\n",
    "\n",
    "from autoencoder import AutoencoderKL\n",
    "from diffusion import (\n",
    "    GaussianDiffusion, beta_schedule)\n",
    "from unet_sd import UNetSD\n",
    "\n",
    "\n",
    "\n",
    "import tempfile\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import json\n",
    "def config_to_dict(config):\n",
    "    cfg = config['model']['model_cfg']\n",
    "    model_args = config['model']['model_args']\n",
    "    cfg['temporal_attention'] = True if cfg['temporal_attention'] == 'True' else False\n",
    "    return cfg, model_args\n",
    "\n",
    "\n",
    "__all__ = ['TextToVideoSynthesis']\n",
    "\n",
    "#set torch seed \n",
    "torch.manual_seed(0)\n",
    "class TextToVideoSynthesis():\n",
    "\n",
    "    def __init__(self, model_dir, *args, **kwargs):\n",
    "\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() \\\n",
    "            else torch.device('cpu')\n",
    "        with open(os.path.join(model_dir, 'configuration.json'), 'r') as f:\n",
    "            config = json.load(f)\n",
    "        cfg, self.model_args = config_to_dict(config)\n",
    "        cfg['temporal_attention'] = True if cfg[\n",
    "            'temporal_attention'] == 'True' else False\n",
    "\n",
    "        # Initialize unet\n",
    "        self.sd_model = UNetSD(\n",
    "            in_dim=cfg['unet_in_dim'],\n",
    "            dim=cfg['unet_dim'],\n",
    "            y_dim=cfg['unet_y_dim'],\n",
    "            context_dim=cfg['unet_context_dim'],\n",
    "            out_dim=cfg['unet_out_dim'],\n",
    "            dim_mult=cfg['unet_dim_mult'],\n",
    "            num_heads=cfg['unet_num_heads'],\n",
    "            head_dim=cfg['unet_head_dim'],\n",
    "            num_res_blocks=cfg['unet_res_blocks'],\n",
    "            attn_scales=cfg['unet_attn_scales'],\n",
    "            dropout=cfg['unet_dropout'],\n",
    "            temporal_attention=cfg['temporal_attention'])\n",
    "\n",
    "        with amp.autocast(enabled=True):\n",
    "            self.sd_model.load_state_dict(torch.load(\n",
    "            osp.join(model_dir, self.model_args['ckpt_unet'])))\n",
    "\n",
    "        self.sd_model.eval()\n",
    "        self.sd_model.to(self.device)\n",
    "        self.sd_model.half()\n",
    "\n",
    "        # Initialize diffusion\n",
    "        betas = beta_schedule(\n",
    "            'linear_sd',\n",
    "            cfg['num_timesteps'],\n",
    "            init_beta=0.00085,\n",
    "            last_beta=0.0120)\n",
    "        self.diffusion = GaussianDiffusion(\n",
    "            betas=betas,\n",
    "            mean_type=cfg['mean_type'],\n",
    "            var_type=cfg['var_type'],\n",
    "            loss_type=cfg['loss_type'],\n",
    "            rescale_timesteps=False)\n",
    "\n",
    "        # Initialize autoencoder\n",
    "        ddconfig = {\n",
    "            'double_z': True,\n",
    "            'z_channels': 4,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 128,\n",
    "            'ch_mult': [1, 2, 4, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [],\n",
    "            'dropout': 0.0\n",
    "        }\n",
    "        self.autoencoder = AutoencoderKL(\n",
    "            ddconfig, 4,\n",
    "            osp.join(model_dir, self.model_args['ckpt_autoencoder']))\n",
    "        if self.model_args['tiny_gpu'] == 1:\n",
    "            self.autoencoder.to('cpu')\n",
    "        else:\n",
    "            self.autoencoder.to(self.device)\n",
    "        self.autoencoder.eval()\n",
    "        self.autoencoder.half()\n",
    "\n",
    "        # Initialize Open clip\n",
    "        self.clip_encoder = FrozenOpenCLIPEmbedder(\n",
    "            version=osp.join(model_dir,\n",
    "                             self.model_args['ckpt_clip']),\n",
    "            layer='penultimate')\n",
    "        if self.model_args['tiny_gpu'] == 1:\n",
    "            self.clip_encoder.to('cpu')\n",
    "        else:\n",
    "            self.clip_encoder.to(self.device)\n",
    "        self.clip_encoder.eval()\n",
    "        self.clip_encoder.half()\n",
    "\n",
    "    def forward(self, input: Dict[str, Any]):\n",
    "        r\"\"\"\n",
    "        The entry function of text to image synthesis task.\n",
    "        1. Using diffusion model to generate the video's latent representation.\n",
    "        2. Using vqgan model (autoencoder) to decode the video's latent representation to visual space.\n",
    "\n",
    "        Args:\n",
    "            input (`Dict[Str, Any]`):\n",
    "                The input of the task\n",
    "        Returns:\n",
    "            A generated video (as pytorch tensor).\n",
    "        \"\"\"\n",
    "        y = input['text_emb']\n",
    "        zero_y = input['text_emb_zero']\n",
    "        context = torch.cat([zero_y, y], dim=0).to(self.device)\n",
    "        # synthesis\n",
    "        with torch.no_grad():\n",
    "            num_sample = 1  # here let b = 1\n",
    "            max_frames = 48\n",
    "            latent_h, latent_w = 32, 64\n",
    "            with amp.autocast(enabled=True):\n",
    "                if input['latent_input'] is None:\n",
    "                    init_latent = torch.randn(num_sample, 4, max_frames, latent_h,\n",
    "                                        latent_w).to(self.device)# 1 was max_frames\n",
    "                    gs=50\n",
    "                    steps = 50\n",
    "                else:   \n",
    "                    init_latent = input['latent_input']\n",
    "                    gs = input['guidance']\n",
    "                    steps = input['steps']\n",
    "                    \n",
    "                x0 = self.diffusion.ddim_sample_loop(\n",
    "                    noise=init_latent,  # shape: b c f h w\n",
    "                    model=self.sd_model,\n",
    "                    model_kwargs=[{\n",
    "                        'y':\n",
    "                        context[1].unsqueeze(0).repeat(num_sample, 1, 1)\n",
    "                    }, {\n",
    "                        'y':\n",
    "                        context[0].unsqueeze(0).repeat(num_sample, 1, 1)\n",
    "                    }],\n",
    "                    guide_scale=gs,\n",
    "                    ddim_timesteps=steps,\n",
    "                    eta=0.0)\n",
    "\n",
    "                scale_factor = 0.18215\n",
    "                video_data = 1. / scale_factor * x0\n",
    "                bs_vd = video_data.shape[0]\n",
    "                video_data = rearrange(video_data, 'b c f h w -> (b f) c h w')\n",
    "                self.autoencoder.to(self.device)\n",
    "\n",
    "                video = torch.zeros((video_data.shape[0], 3, latent_h*8, latent_w*8))\n",
    "                for i in range(0, video_data.shape[0]):\n",
    "                    video[i] = self.autoencoder.decode(video_data[i].unsqueeze(0)).detach().cpu().squeeze()\n",
    "                if self.model_args['tiny_gpu'] == 1:\n",
    "                    self.autoencoder.to('cpu')\n",
    "                video_data = rearrange(\n",
    "                    video, '(b f) c h w -> b c f h w', b=bs_vd)\n",
    "        return video_data.type(torch.float32), x0\n",
    "\n",
    "\n",
    "class FrozenOpenCLIPEmbedder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Uses the OpenCLIP transformer encoder for text\n",
    "    \"\"\"\n",
    "    LAYERS = ['last', 'penultimate']\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='ViT-H-14',\n",
    "                 version='open_clip_pytorch_model.bin',\n",
    "                 device='cuda',\n",
    "                 max_length=77,\n",
    "                 freeze=True,\n",
    "                 layer='last'):\n",
    "        super().__init__()\n",
    "        assert layer in self.LAYERS\n",
    "        model, _, _ = open_clip.create_model_and_transforms(\n",
    "            arch, device=torch.device('cpu'), pretrained=version)\n",
    "        del model.visual\n",
    "        self.model = model\n",
    "\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        if freeze:\n",
    "            self.freeze()\n",
    "        self.layer = layer\n",
    "        if self.layer == 'last':\n",
    "            self.layer_idx = 0\n",
    "        elif self.layer == 'penultimate':\n",
    "            self.layer_idx = 1\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.model = self.model.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = open_clip.tokenize(text)\n",
    "        z = self.encode_with_transformer(tokens.to(self.device))\n",
    "        return z\n",
    "\n",
    "    def encode_with_transformer(self, text):\n",
    "        x = self.model.token_embedding(text)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.model.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.model.ln_final(x)\n",
    "        return x\n",
    "\n",
    "    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n",
    "        for i, r in enumerate(self.model.transformer.resblocks):\n",
    "            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n",
    "                break\n",
    "            x = r(x, attn_mask=attn_mask)\n",
    "        return x\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self(text)\n",
    "\n",
    "class TextToVideoSynthesisPipeline():\n",
    "\n",
    "    def __init__(self, model: str, **kwargs):\n",
    "        self.model = TextToVideoSynthesis(model, **kwargs)\n",
    "\n",
    "    def preprocess(self, input, **preprocess_params) -> Dict[str, Any]:\n",
    "        self.model.clip_encoder.to(self.model.device)\n",
    "        text_emb = self.model.clip_encoder(input['text'])\n",
    "        text_emb_zero = self.model.clip_encoder('')\n",
    "        if self.model.config.model.model_args.tiny_gpu == 1:\n",
    "            self.model.clip_encoder.to('cpu')\n",
    "        return {'text_emb': text_emb, 'text_emb_zero': text_emb_zero}\n",
    "\n",
    "    def forward(self, input: Dict[str, Any],\n",
    "                **forward_params) -> Dict[str, Any]:\n",
    "        video, latent = self.model.forward(input)\n",
    "        return {'video': video}, latent\n",
    "\n",
    "    def postprocess(self, inputs: Dict[str, Any],\n",
    "                    **post_params) -> Dict[str, Any]:\n",
    "        video = tensor2vid(inputs['video'])\n",
    "        output_video_path = post_params.get('output_video', None)\n",
    "        if output_video_path is None:\n",
    "            output_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        h, w, c = video[0].shape\n",
    "        video_writer = cv2.VideoWriter(\n",
    "            output_video_path, fourcc, fps=8, frameSize=(w, h))\n",
    "        for i in range(len(video)):\n",
    "            img = cv2.cvtColor(video[i], cv2.COLOR_RGB2BGR)\n",
    "            video_writer.write(img)\n",
    "        return output_video_path\n",
    "\n",
    "def tensor2vid(video, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "    mean = torch.tensor(\n",
    "        mean, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw\n",
    "    std = torch.tensor(\n",
    "        std, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw\n",
    "    video = video.mul_(std).add_(mean)  # unnormalize back to [0,1]\n",
    "    video.clamp_(0, 1)\n",
    "    images = rearrange(video, 'i c f h w -> f h (i w) c')\n",
    "    images = images.unbind(dim=0)\n",
    "    images = [(image.numpy() * 255).astype('uint8')\n",
    "              for image in images]  # f h w c\n",
    "    return images\n",
    "\n",
    "\n",
    "pipeline = TextToVideoSynthesisPipeline('text-to-video-synthesis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pipeline.preprocess({'text': 'A cow running a marathon'})\n",
    "inp['latent_input'] = None\n",
    "vid, latent_inp = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = {}\n",
    "inp['text_emb'], inp['text_emb_zero'] = pipeline.preprocess({'text': 'A cow running a marathon in hell'}).values()\n",
    "inp['latent_input'] = latent_inp*0.1 + 0.9*torch.randn(1, 4, 48, 32,64).to('cuda')#ok\n",
    "inp['guidance'] = 50\n",
    "inp['steps'] = 50\n",
    "vid, _ = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='dog.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "#open  vtv/chicken.mp4\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "frames = []\n",
    "cap = cv2.VideoCapture('vtv/chicken.mp4')\n",
    "#open at 8 fps\n",
    "cap.set(cv2.CAP_PROP_FPS, 8)\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    #convert to rgb    \n",
    "\n",
    "    if ret == True:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame).crop((0, 0, 512, 256)))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "f = frames[::3][:48]\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "encoded_frames = []\n",
    "\n",
    "for frame in f:\n",
    "    frame = torch.tensor(np.array(f[0])).unsqueeze(0).transpose(1,3).transpose(2,3)\n",
    "    o = model.autoencoder.to('cuda').encode(frame.cuda().half())\n",
    "    o = o.sample().unsqueeze(2).cpu().detach()\n",
    "    #normalize o based on mean and std of the latent space\n",
    "    o = (o - o.mean()) / o.std()#the latents are good once normalized\n",
    "    encoded_frames.append(o)\n",
    "\n",
    "latent_inp = torch.cat(encoded_frames, dim=2).cuda().half()\n",
    "\n",
    "model.autoencoder.to('cpu')\n",
    "del o\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = {}\n",
    "inp['text_emb'], inp['text_emb_zero'] = pipeline.preprocess({'text': 'A person eating a chicken'}).values()\n",
    "inp['latent_input'] = latent_inp*0.01 + 0.99*torch.randn(1, 4, 48, 32,64).to('cuda')#ok\n",
    "inp['guidance'] = 10\n",
    "inp['steps'] = 50\n",
    "vid, _ = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='dog.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pipeline.preprocess({'text': 'A cow running a marathon'})\n",
    "inp['latent_input'] = None\n",
    "vid, latent_inp = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pipeline.preprocess({'text': 'A cow running a marathon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"mask.PNG\").resize((64, 32)))\n",
    "init_latent = torch.randn(1, 4, 48, 32,64)\n",
    "init_latent[:, :, :, mask > 0] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 151882.87it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.70it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.68it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.65it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.62it/s]\n",
      " 20%|██        | 10/50 [00:01<00:05,  7.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m mask_image \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mresize((\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m))\n\u001b[1;32m     30\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mA cow running a marathon\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 31\u001b[0m image \u001b[39m=\u001b[39m pipe(prompt\u001b[39m=\u001b[39;49mprompt, image\u001b[39m=\u001b[39;49minit_image, mask_image\u001b[39m=\u001b[39;49mmask_image)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mresize((\u001b[39m512\u001b[39m, \u001b[39m256\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[39m#save image to file\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py:811\u001b[0m, in \u001b[0;36mStableDiffusionInpaintPipeline.__call__\u001b[0;34m(self, prompt, image, mask_image, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, nrompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps)\u001b[0m\n\u001b[1;32m    808\u001b[0m     noise_pred \u001b[39m=\u001b[39m noise_pred_uncond \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39m (noise_pred_text \u001b[39m-\u001b[39m noise_pred_uncond)\n\u001b[1;32m    810\u001b[0m \u001b[39m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[0;32m--> 811\u001b[0m latents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscheduler\u001b[39m.\u001b[39;49mstep(noise_pred, t, latents, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_step_kwargs)\u001b[39m.\u001b[39mprev_sample\n\u001b[1;32m    813\u001b[0m \u001b[39m# call the callback, if provided\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(timesteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m ((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m>\u001b[39m num_warmup_steps \u001b[39mand\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39morder \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:221\u001b[0m, in \u001b[0;36mPNDMScheduler.step\u001b[0;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_prk(model_output\u001b[39m=\u001b[39mmodel_output, timestep\u001b[39m=\u001b[39mtimestep, sample\u001b[39m=\u001b[39msample, return_dict\u001b[39m=\u001b[39mreturn_dict)\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_plms(model_output\u001b[39m=\u001b[39;49mmodel_output, timestep\u001b[39m=\u001b[39;49mtimestep, sample\u001b[39m=\u001b[39;49msample, return_dict\u001b[39m=\u001b[39;49mreturn_dict)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:337\u001b[0m, in \u001b[0;36mPNDMScheduler.step_plms\u001b[0;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     model_output \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m24\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39m55\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m59\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m37\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m])\n\u001b[0;32m--> 337\u001b[0m prev_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_prev_sample(sample, timestep, prev_timestep, model_output)\n\u001b[1;32m    338\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:373\u001b[0m, in \u001b[0;36mPNDMScheduler._get_prev_sample\u001b[0;34m(self, sample, timestep, prev_timestep, model_output)\u001b[0m\n\u001b[1;32m    371\u001b[0m alpha_prod_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas_cumprod[timestep]\n\u001b[1;32m    372\u001b[0m alpha_prod_t_prev \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas_cumprod[prev_timestep] \u001b[39mif\u001b[39;00m prev_timestep \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_alpha_cumprod\n\u001b[0;32m--> 373\u001b[0m beta_prod_t \u001b[39m=\u001b[39m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m alpha_prod_t\n\u001b[1;32m    374\u001b[0m beta_prod_t_prev \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha_prod_t_prev\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mprediction_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mv_prediction\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:37\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f, assigned\u001b[39m=\u001b[39massigned)\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[39m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m         \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     38\u001b[0m             \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "vid = cv2.VideoCapture('test.mp4')\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.safety_checker = None\n",
    "pipe = pipe.to(\"cuda\")\n",
    "ret, frame = vid.read()\n",
    "i = 0\n",
    "while ret:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = Image.fromarray(frame)\n",
    "\n",
    "    mask = np.array(Image.open(\"mask_hard.PNG\"))\n",
    "    mask = Image.fromarray(mask//2)\n",
    "\n",
    "\n",
    "    init_image = frame.resize((512, 512))\n",
    "    mask_image = mask.resize((512, 512))\n",
    "\n",
    "    prompt = 'A cow running a marathon'\n",
    "    image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n",
    "    image = image.resize((512, 256))\n",
    "    #save image to file\n",
    "    image.save(f'cowupscale/{i}.png')\n",
    "\n",
    "    ret, frame = vid.read()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# mask = np.array(Image.open(\"mask.PNG\").resize((64, 32)))\n",
    "# init_latent[:, :, :, mask > 0] *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "mask = torch.tensor(np.array(Image.open(\"mask.PNG\")))\n",
    "m = mask.unsqueeze(0).transpose(1,3).transpose(2,3)\n",
    "aut = model.autoencoder.to('cuda')\n",
    "o = aut.encode(m.cuda().half())\n",
    "o.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "Fetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 233016.89it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 64 but got size 32 for tensor number 2 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFace of a yellow cat, high resolution, sitting on a park bench\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m image \u001b[39m=\u001b[39m pipe(prompt\u001b[39m=\u001b[39;49mprompt, image\u001b[39m=\u001b[39;49minit_image, mask_image\u001b[39m=\u001b[39;49mmask_image)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py:855\u001b[0m, in \u001b[0;36mStableDiffusionInpaintPipeline.__call__\u001b[0;34m(self, prompt, image, mask_image, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[39m# concat latents, mask, masked_image_latents in the channel dimension\u001b[39;00m\n\u001b[1;32m    854\u001b[0m latent_model_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[0;32m--> 855\u001b[0m latent_model_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([latent_model_input, mask, masked_image_latents], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    857\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[1;32m    858\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet(latent_model_input, t, encoder_hidden_states\u001b[39m=\u001b[39mprompt_embeds)\u001b[39m.\u001b[39msample\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 64 but got size 32 for tensor number 2 in the list."
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "     response = requests.get(url)\n",
    "     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = download_image(img_url).resize((512, 256))\n",
    "mask_image = download_image(mask_url).resize((512, 256))\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "     \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16\n",
    " )\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n",
    "image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nepyope/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-27 19:16:53.371368: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib:/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib\n",
      "2023-03-27 19:16:53.371436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib:/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib\n",
      "2023-03-27 19:16:53.371442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "unet/diffusion_pytorch_model.safetensors not found\n",
      "Fetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 159403.48it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 64 but got size 32 for tensor number 2 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mA cow running a marathon\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39m#image and mask_image should be PIL images.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m#The mask structure is white for inpainting and black for keeping as is\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m image \u001b[39m=\u001b[39m pipe(prompt\u001b[39m=\u001b[39;49mprompt, image\u001b[39m=\u001b[39;49mframe, mask_image\u001b[39m=\u001b[39;49mmask)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m image\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./out.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py:855\u001b[0m, in \u001b[0;36mStableDiffusionInpaintPipeline.__call__\u001b[0;34m(self, prompt, image, mask_image, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[39m# concat latents, mask, masked_image_latents in the channel dimension\u001b[39;00m\n\u001b[1;32m    854\u001b[0m latent_model_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[0;32m--> 855\u001b[0m latent_model_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([latent_model_input, mask, masked_image_latents], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    857\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[1;32m    858\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet(latent_model_input, t, encoder_hidden_states\u001b[39m=\u001b[39mprompt_embeds)\u001b[39m.\u001b[39msample\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 64 but got size 32 for tensor number 2 in the list."
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    ")\n",
    "\n",
    "\n",
    "vid = cv2.VideoCapture('test.mp4')\n",
    "for i in range(15):\n",
    "    ret, frame = vid.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "fra\n",
    "#open mask as rgb\n",
    "mask = np.array(Image.open(\"mask_256_512.PNG\"))\n",
    "\n",
    "\n",
    "prompt = \"A cow running a marathon\"\n",
    "#image and mask_image should be PIL images.\n",
    "#The mask structure is white for inpainting and black for keeping as is\n",
    "image = pipe(prompt=prompt, image=frame, mask_image=mask).images[0]\n",
    "image.save(\"./out.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(frame)[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the first image from test.mp4\n",
    "import cv2\n",
    "\n",
    "vid = cv2.VideoCapture('test.mp4')\n",
    "for i in range(15):\n",
    "    ret, frame = vid.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open mask.PNG\n",
    "mask = Image.open('mask.PNG')\n",
    "#only keep color blue\n",
    "mask = np.array(mask)\n",
    "mask = mask[:,:,2]\n",
    "mask = Image.fromarray(mask)\n",
    "#only keep 255 white\n",
    "mask = np.array(mask)\n",
    "mask[mask<255] = 0\n",
    "mask = Image.fromarray(mask)\n",
    "mask.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[146, 155,  99, 255],\n",
       "        [146, 155,  99, 255],\n",
       "        [146, 155,  99, 255],\n",
       "        ...,\n",
       "        [143, 137, 112, 255],\n",
       "        [143, 137, 112, 255],\n",
       "        [143, 137, 112, 255]],\n",
       "\n",
       "       [[147, 156, 100, 255],\n",
       "        [147, 156, 100, 255],\n",
       "        [147, 156, 100, 255],\n",
       "        ...,\n",
       "        [155, 149, 124, 255],\n",
       "        [155, 149, 124, 255],\n",
       "        [155, 149, 124, 255]],\n",
       "\n",
       "       [[147, 153, 101, 255],\n",
       "        [147, 153, 101, 255],\n",
       "        [147, 153, 101, 255],\n",
       "        ...,\n",
       "        [156, 151, 123, 255],\n",
       "        [156, 151, 123, 255],\n",
       "        [156, 151, 123, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[184, 165, 183, 255],\n",
       "        [184, 165, 183, 255],\n",
       "        [184, 165, 183, 255],\n",
       "        ...,\n",
       "        [187, 155, 180, 255],\n",
       "        [186, 154, 181, 255],\n",
       "        [184, 152, 179, 255]],\n",
       "\n",
       "       [[186, 167, 185, 255],\n",
       "        [186, 167, 185, 255],\n",
       "        [186, 167, 185, 255],\n",
       "        ...,\n",
       "        [198, 166, 191, 255],\n",
       "        [197, 165, 190, 255],\n",
       "        [194, 162, 187, 255]],\n",
       "\n",
       "       [[198, 179, 197, 255],\n",
       "        [198, 179, 197, 255],\n",
       "        [198, 179, 197, 255],\n",
       "        ...,\n",
       "        [202, 170, 195, 255],\n",
       "        [204, 172, 197, 255],\n",
       "        [202, 170, 195, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pipeline.preprocess({'text': 'A dragon breathing fire'})\n",
    "inp['latent_input'] = None\n",
    "vid = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pipeline.preprocess({'text': 'A dog jumping up and down'})\n",
    "inp['latent_input'] = None\n",
    "vid = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp['latent_input'] = latent_inp*0.2 + 0.8*torch.randn(1, 4, 16, 32,32).to('cuda')#ok\n",
    "inp['guidance'] = 10\n",
    "inp['steps'] = 50\n",
    "vid, _ = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='dog.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(1, 4, 16, 32,32).to('cuda')\n",
    "c = torch.cat((noise[:,:,:8,:,:]*0.7+latent_inp[:,:,8:,:,:]*0.3, noise[:,:,8:,:,:]), dim=2)#70% of the first half of the latent code is from the previous frame, 30% is from the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(1, 4, 16, 32,32).to('cuda')\n",
    "for i in range(10):\n",
    "    noise[:, :, i, :, :] = latent_inp[:, :, i, :, :]*(1/(i+1))+noise[:, :, i, :, :]*(1-1/(i+1))#gradually add noise to the first half of the latent code\n",
    "\n",
    "c = noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp['text_embed'], inp['text_input'] = pipeline.preprocess({'text': 'A brown dog eating a log in a garden'})\n",
    "inp['latent_input'] = c\n",
    "inp['guidance'] = 5\n",
    "inp['steps'] = 50\n",
    "vid, _ = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='dog_continue.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
