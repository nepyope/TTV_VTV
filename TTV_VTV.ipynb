{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download models from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P models https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/resolve/main/VQGAN_autoencoder.pth \\\n",
    "                https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/resolve/main/open_clip_pytorch_model.bin \\\n",
    "                https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/resolve/main/text2video_pytorch_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nepyope/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-28 02:27:00.680151: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 02:27:02.676270: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nepyope/.local/lib/python3.10/site-packages/cv2/../../lib64::/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib:/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib\n",
      "2023-03-28 02:27:02.676392: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nepyope/.local/lib/python3.10/site-packages/cv2/../../lib64::/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib:/home/nepyope/.local/lib/python3.10/site-packages/nvidia/cuda_runtime/lib\n",
      "2023-03-28 02:27:02.676401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/nepyope/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/nepyope/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from os import path as osp\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import cv2\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from einops import rearrange\n",
    "from autoencoder import AutoencoderKL\n",
    "from clip import FrozenOpenCLIPEmbedder\n",
    "from diffusion import (GaussianDiffusion, beta_schedule)\n",
    "from unet_sd import UNetSD\n",
    "import tempfile\n",
    "import cv2\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "#set torch seed \n",
    "torch.manual_seed(0)\n",
    "\n",
    "class TextToVideoSynthesis():\n",
    "\n",
    "    def __init__(self, model_dir):\n",
    "\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() \\\n",
    "            else torch.device('cpu')\n",
    "\n",
    "        # Initialize unet\n",
    "        self.sd_model = UNetSD(\n",
    "            in_dim=4,\n",
    "            dim=320,\n",
    "            y_dim=768,\n",
    "            context_dim=1024,\n",
    "            out_dim=4,\n",
    "            dim_mult=[1, 2, 4, 4],\n",
    "            num_heads=8,\n",
    "            head_dim=64,\n",
    "            num_res_blocks=2,\n",
    "            attn_scales=[1, 0.5, 0.25],\n",
    "            dropout=0.1,\n",
    "            temporal_attention=True)\n",
    "\n",
    "        with amp.autocast(enabled=True):\n",
    "            self.sd_model.load_state_dict(torch.load(\n",
    "            osp.join(model_dir, \"text2video_pytorch_model.pth\")))\n",
    "\n",
    "        self.sd_model.eval()\n",
    "        self.sd_model.to(self.device)\n",
    "        self.sd_model.half()\n",
    "\n",
    "        # Initialize diffusion\n",
    "        betas = beta_schedule(\n",
    "            'linear_sd',\n",
    "            1000,\n",
    "            init_beta=0.00085,\n",
    "            last_beta=0.0120)\n",
    "        self.diffusion = GaussianDiffusion(\n",
    "            betas=betas,\n",
    "            mean_type=\"eps\",\n",
    "            var_type=\"fixed_small\",\n",
    "            loss_type=\"mse\",\n",
    "            rescale_timesteps=False)\n",
    "\n",
    "        # Initialize autoencoder\n",
    "        ddconfig = {\n",
    "            'double_z': True,\n",
    "            'z_channels': 4,\n",
    "            'resolution': 256,\n",
    "            'in_channels': 3,\n",
    "            'out_ch': 3,\n",
    "            'ch': 128,\n",
    "            'ch_mult': [1, 2, 4, 4],\n",
    "            'num_res_blocks': 2,\n",
    "            'attn_resolutions': [],\n",
    "            'dropout': 0.0\n",
    "        }\n",
    "        self.autoencoder = AutoencoderKL(\n",
    "            ddconfig, 4,\n",
    "            osp.join(model_dir, \"VQGAN_autoencoder.pth\"))\n",
    "        self.autoencoder.to('cpu')\n",
    "        self.autoencoder.eval()\n",
    "        self.autoencoder.half()\n",
    "\n",
    "        # Initialize Open clip\n",
    "        self.clip_encoder = FrozenOpenCLIPEmbedder(\n",
    "            version=osp.join(model_dir,\n",
    "                             \"open_clip_pytorch_model.bin\"),\n",
    "            layer='penultimate')\n",
    "        self.clip_encoder.to('cpu')\n",
    "        self.clip_encoder.eval()\n",
    "        self.clip_encoder.half()\n",
    "\n",
    "    def forward(self, input: Dict[str, Any]):\n",
    "\n",
    "        y = input['text_emb']\n",
    "        zero_y = input['text_emb_zero']\n",
    "        context = torch.cat([zero_y, y], dim=0).to(self.device)\n",
    "        # synthesis\n",
    "        with torch.no_grad():\n",
    "            num_sample = 1  # here let b = 1\n",
    "            max_frames = 48\n",
    "            latent_h, latent_w = 32, 64\n",
    "            with amp.autocast(enabled=True):\n",
    "                if input['latent_input'] is None:\n",
    "                    init_latent = torch.randn(num_sample, 4, max_frames, latent_h,\n",
    "                                        latent_w).to(self.device)# 1 was max_frames\n",
    "                    gs=50\n",
    "                    steps = 50\n",
    "                else:   \n",
    "                    init_latent = input['latent_input']\n",
    "                    gs = input['guidance']\n",
    "                    steps = input['steps']\n",
    "                    \n",
    "                x0 = self.diffusion.ddim_sample_loop(\n",
    "                    noise=init_latent,  # shape: b c f h w\n",
    "                    model=self.sd_model,\n",
    "                    model_kwargs=[{\n",
    "                        'y':\n",
    "                        context[1].unsqueeze(0).repeat(num_sample, 1, 1)\n",
    "                    }, {\n",
    "                        'y':\n",
    "                        context[0].unsqueeze(0).repeat(num_sample, 1, 1)\n",
    "                    }],\n",
    "                    guide_scale=gs,\n",
    "                    ddim_timesteps=steps,\n",
    "                    eta=0.0)\n",
    "\n",
    "                scale_factor = 0.18215\n",
    "                video_data = 1. / scale_factor * x0\n",
    "                bs_vd = video_data.shape[0]\n",
    "                video_data = rearrange(video_data, 'b c f h w -> (b f) c h w')\n",
    "                self.autoencoder.to(self.device)\n",
    "\n",
    "                video = torch.zeros((video_data.shape[0], 3, latent_h*8, latent_w*8))\n",
    "                for i in range(0, video_data.shape[0]):\n",
    "                    video[i] = self.autoencoder.decode(video_data[i].unsqueeze(0)).detach().cpu().squeeze()\n",
    "                self.autoencoder.to('cpu')\n",
    "                video_data = rearrange(\n",
    "                    video, '(b f) c h w -> b c f h w', b=bs_vd)\n",
    "        return video_data.type(torch.float32), x0\n",
    "\n",
    "class TextToVideoSynthesisPipeline():\n",
    "\n",
    "    def __init__(self, model: str, **kwargs):\n",
    "        self.model = TextToVideoSynthesis(model, **kwargs)\n",
    "\n",
    "    def preprocess(self, input) -> Dict[str, Any]:\n",
    "        self.model.clip_encoder.to(self.model.device)\n",
    "        text_emb = self.model.clip_encoder(input['text'])\n",
    "        text_emb_zero = self.model.clip_encoder('')\n",
    "        self.model.clip_encoder.to('cpu')\n",
    "        return {'text_emb': text_emb, 'text_emb_zero': text_emb_zero}\n",
    "\n",
    "    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        video, latent = self.model.forward(input)\n",
    "        return {'video': video}, latent\n",
    "\n",
    "    def postprocess(self, inputs: Dict[str, Any],\n",
    "                    **post_params) -> Dict[str, Any]:\n",
    "        video = tensor2vid(inputs['video'])\n",
    "        output_video_path = post_params.get('output_video', None)\n",
    "        if output_video_path is None:\n",
    "            output_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        h, w, c = video[0].shape\n",
    "        video_writer = cv2.VideoWriter(\n",
    "            output_video_path, fourcc, fps=8, frameSize=(w, h))\n",
    "        for i in range(len(video)):\n",
    "            img = cv2.cvtColor(video[i], cv2.COLOR_RGB2BGR)\n",
    "            video_writer.write(img)\n",
    "        return output_video_path\n",
    "\n",
    "def tensor2vid(video, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "    mean = torch.tensor(\n",
    "        mean, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw\n",
    "    std = torch.tensor(\n",
    "        std, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw\n",
    "    video = video.mul_(std).add_(mean)  # unnormalize back to [0,1]\n",
    "    video.clamp_(0, 1)\n",
    "    images = rearrange(video, 'i c f h w -> f h (i w) c')\n",
    "    images = images.unbind(dim=0)\n",
    "    images = [(image.numpy() * 255).astype('uint8')\n",
    "              for image in images]  # f h w c\n",
    "    return images\n",
    "\n",
    "\n",
    "pipeline = TextToVideoSynthesisPipeline('models')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-to-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pipeline.preprocess({'text': 'A cow running a marathon'})\n",
    "inp['latent_input'] = None\n",
    "vid, latent_inp = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='TTV.mp4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video-to-Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = {}\n",
    "inp['text_emb'], inp['text_emb_zero'] = pipeline.preprocess({'text': 'A cow running a marathon in hell'}).values()\n",
    "inp['latent_input'] = latent_inp*0.1 + 0.9*torch.randn(1, 4, 48, 32,64).to('cuda')#ok\n",
    "inp['guidance'] = 50\n",
    "inp['steps'] = 50\n",
    "vid, _ = pipeline.forward(inp)\n",
    "out = pipeline.postprocess(vid, output_video='VTV.mp4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use real video as input (doesn't work (YET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "#open  vtv/chicken.mp4\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "frames = []\n",
    "cap = cv2.VideoCapture('vtv/chicken.mp4')\n",
    "#open at 8 fps\n",
    "cap.set(cv2.CAP_PROP_FPS, 8)\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    #convert to rgb    \n",
    "\n",
    "    if ret == True:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame).crop((0, 0, 512, 256)))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "f = frames[::3][:48]\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "encoded_frames = []\n",
    "\n",
    "for frame in f:\n",
    "    frame = torch.tensor(np.array(f[0])).unsqueeze(0).transpose(1,3).transpose(2,3)\n",
    "    o = pipeline.model.autoencoder.to('cuda').encode(frame.cuda().half())\n",
    "    o = o.sample().unsqueeze(2).cpu().detach()\n",
    "    #normalize o based on mean and std of the latent space\n",
    "    o = (o - o.mean()) / o.std()#the latents are good once normalized\n",
    "    encoded_frames.append(o)\n",
    "\n",
    "latent_inp = torch.cat(encoded_frames, dim=2).cuda().half()\n",
    "\n",
    "pipeline.model.autoencoder.to('cpu')\n",
    "del o\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
